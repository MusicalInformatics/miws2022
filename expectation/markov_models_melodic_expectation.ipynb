{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35620ff",
   "metadata": {},
   "source": [
    "# Melodic Expectation with Markov Models\n",
    "\n",
    "In this notebook we will look at Markov Chains for modeling musical expectation.\n",
    "We have already seen a Markov Model in the class on key estimation with HMMs (Hidden Markov Models).\n",
    "\n",
    "## Information theoretic measures of musical expectation\n",
    "\n",
    "* Information Content: Unexpectedness of a musical event\n",
    "\n",
    "$$\\text{IC}(\\mathbf{x}_n\\mid \\mathbf{x}_{n-1}, \\dots) = -\\log_2 p(\\mathbf{x}_n \\mid \\mathbf{x}_{n-1}, \\dots )$$\n",
    "\n",
    "* Entropy: How uncertain is a musical event\n",
    "\n",
    "$$\\text{H}(\\mathbf{x}_{n-1:1})= \\sum_{i \\in \\mathbf{A}} p(\\mathbf{x}_i \\mid \\mathbf{x}_{n-1}, \\dots) \\text{IC}(\\mathbf{x}_i\\mid \\mathbf{x}_{n-1}, \\dots)$$\n",
    "\n",
    "where $\\mathbf{A}$ is the set of possible states that $\\mathbf{x}$ can take.\n",
    "\n",
    "## First order Markov Chains\n",
    "\n",
    "The conditional distribution of each variable is independent of all previous observations except for the most recent\n",
    "\n",
    "<div>\n",
    "<img src=\"img/first_order_markov.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "$$p(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) = p(\\mathbf{x}_1)\\prod_{n=2}^{N}p(\\mathbf{x}_nÂ \\mid \\mathbf{x}_{n-1})$$\n",
    "\n",
    "The information content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ea261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Uncomment this line if the kernel keeps crashing\n",
    "# See https://stackoverflow.com/a/53014308\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import numpy as np\n",
    "import partitura as pt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea836bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import load_data\n",
    "\n",
    "# To filter out short melodies by the minimum number of notes that a sequence should have\n",
    "\n",
    "min_seq_len = 10\n",
    "sequences = load_data(min_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598ae3b",
   "metadata": {},
   "source": [
    "## Tasks 1; data loading & preparing:\n",
    "1. check out the content of the variable \"sequences\", if unclear have a look at the loading function.\n",
    "2. which musical texture do these sequences exhibit? (https://en.wikipedia.org/wiki/Texture_(music))\n",
    "3. write a function to derive sequences of pitches from this data.\n",
    "4. write a function to derive sequences of durations from this data. Modify this to compute inter onset intervals (IOIs; the time between two consecutive onsets). Can you encode rests as well by comparing duration with IOI? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a17d83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = list()\n",
    "for seq in sequences:\n",
    "    # pitch\n",
    "    # features.append(seq[\"pitch\"])\n",
    "    # pitch class\n",
    "    features.append(np.mod(seq[\"pitch\"], 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923be4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "enc.fit(np.hstack(features))\n",
    "encoded_sequences = [enc.transform(seq) for seq in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a8b9b",
   "metadata": {},
   "source": [
    "## Tasks 2; data exploration:\n",
    "\n",
    "1. compute and draw a histogram of pitches. Modify this to show pitch classes!\n",
    "2. compute and draw a histogram of IOIs. The input MIDI files are deadpan, i.e. the IOIs in seconds correspond to the notated duration exactly. Look through the IOIs and make an educated guess for some smallest float time unit that could serve as integer smallest time division. Encode the IOIs as multiples of this smallest integer. Which multiples make musical sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ede8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "alphabet_size = len(enc.classes_)\n",
    "alphabet = enc.classes_\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "print(f\"Unique elements {alphabet_size}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(np.hstack(encoded_sequences), bins=alphabet_size, range=(0,alphabet_size), color='firebrick')\n",
    "ax.set_xticks(np.arange(alphabet_size) + 0.5)\n",
    "ax.set_xticklabels(alphabet)\n",
    "ax.set_ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674b16f",
   "metadata": {},
   "source": [
    "## Tasks 3; A Markov Chain:\n",
    "\n",
    "1. choose a data type to model: pitch, pitch class, IOIs, or durations (including or without an encoding for rests). Concatenate all the sequences into one long data sequence.\n",
    "\n",
    "2. You have now a sequence **X** of symbols from an alphabet **A** (set of possible symbols of your chosen data type):\n",
    "\n",
    "$$ \\mathbf{X} = \\{\\mathbf{x_0}, \\dots, \\mathbf{x_n} \\mid \\mathbf{x}_{i} \\in  \\mathbf{A} \\forall i \\in 0, \\dots, n \\}$$\n",
    "\n",
    "Compute the empirical conditional probability of seeing any symbol after just having seen any other:\n",
    "\n",
    "$$ p(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}}) $$\n",
    "\n",
    "What is the dimensionality of this probability  given $\\lvert A \\rvert = d $? Do you recall what this probability was called in the context of HMMs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5368b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def maximum_likelihood_fom(sequences, alphabet_size):\n",
    "    probs = np.zeros((alphabet_size, alphabet_size)) + 1e-6\n",
    "    for seq in sequences:\n",
    "        for (p1, p2) in zip(seq[:-1],seq[1:]):\n",
    "            probs[p1, p2] += 1\n",
    "    probsum = np.sum(probs, axis = 1)\n",
    "    normalized_distribution = (probs.T/probsum).T\n",
    "    return normalized_distribution\n",
    "\n",
    "transition_probabilities = maximum_likelihood_fom(encoded_sequences, alphabet_size)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "a = ax.matshow(transition_probabilities, aspect='equal', cmap='BuPu')\n",
    "ax.set_xticks(range(alphabet_size))\n",
    "ax.set_xticklabels(enc.classes_)\n",
    "ax.set_yticks(range(alphabet_size))\n",
    "ax.set_yticklabels(enc.classes_)\n",
    "\n",
    "plt.colorbar(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbd81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def probability_of_sequence(sequence, transition_probs=transition_probabilities, alphabet_size=alphabet_size):\n",
    "    \"\"\"\n",
    "    p(x_1,...,x_n)\n",
    "    \"\"\"\n",
    "    p_0 = 1 / alphabet_size\n",
    "    transitions = np.prod([transition_probs[i, j] for i, j in zip(sequence[1:],sequence[:-1])])\n",
    "    prob = p_0 * transitions\n",
    "    return prob\n",
    "\n",
    "def information_content_fom(sequence, transition_probs=transition_probabilities, alphabet_size=alphabet_size):\n",
    "    \"\"\"\n",
    "    For first order Markov models\n",
    "    \n",
    "    IC(x_n|x_{n-1},...,x_1) = IC(x_n|x_{n-1}) = - log2(p(x_n | x_{n-1}))\n",
    "    \"\"\"    \n",
    "    ic = - np.log2(transition_probs[sequence[-1], sequence[-2]])\n",
    "    return ic\n",
    "\n",
    "def entropy_fom(sequence, alphabet=alphabet, transition_probs=transition_probabilities):\n",
    "    \"\"\"\n",
    "    Entropy for first order Markov model\n",
    "    \"\"\"\n",
    "    entropy_components = []\n",
    "    for al in alphabet:\n",
    "        in_seq = np.zeros_like(sequence)\n",
    "        in_seq[:-1] = sequence[:-1]\n",
    "        in_seq[-1] = al\n",
    "        \n",
    "        ic = information_content_fom(in_seq, transition_probs, alphabet_size=len(alphabet))\n",
    "        prob = transition_probs[al, sequence[-2]]\n",
    "        \n",
    "        entropy_components.append(ic * prob)\n",
    "    entropy = np.sum(entropy_components)\n",
    "    \n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44c602",
   "metadata": {},
   "source": [
    "## Tasks 4; Markov Chain Generation:\n",
    "\n",
    "1. By computing the probability $ \\mathbb{P}(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}}) $ in task 3 you have fully specified a discrete-time finite state space Markov Chain model (https://en.wikipedia.org/wiki/Discrete-time_Markov_chain)! Given an initial symbol \"s_0\", you can generate the subsequent symbols by sampling from the conditional probability distribution\n",
    "\n",
    "$$ p(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}} = \\mathbf{s_{0}}) $$\n",
    "\n",
    "Write a function that samples from a finite state space given an input probability distribution.\n",
    "\n",
    "2. Use the previously defined function and the Markov Chain to write a sequence generator based on an initial symbol.\n",
    "3. Start several \"walkers\", i.e. sampled/generated sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bcf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from partitura.utils.synth import synthesize, SAMPLE_RATE\n",
    "import IPython.display as ipd\n",
    "\n",
    "def sample(distribution):\n",
    "    cs = distribution.cumsum()\n",
    "    samp = np.random.rand(1)\n",
    "    return list(samp < cs).index(True)\n",
    "    \n",
    "\n",
    "def generate(start = 0, length = 100):\n",
    "    melody = [start]\n",
    "    for k in range(length - 1):\n",
    "        melody.append(sample(transition_probabilities[melody[-1],:]))\n",
    "    return np.array(melody)\n",
    "\n",
    "\n",
    "\n",
    "def synthesize_generated_sequence(generated_sequence):\n",
    "    note_array = np.zeros(\n",
    "        len(generated_sequence),\n",
    "        dtype=[\n",
    "            ('onset_sec', 'f4'),\n",
    "            ('duration_sec', 'f4'),\n",
    "            ('pitch', 'i4'),\n",
    "            ('velocity', 'i4'),\n",
    "            ('id', 'U256')\n",
    "        ]\n",
    "    )\n",
    "    note_array['pitch'] = generated_sequence + 60\n",
    "    note_array['onset_sec'] = np.arange(len(generated_sequence)) * 0.5\n",
    "    note_array['duration_sec'] += 0.5\n",
    "    note_array['velocity'] = 64\n",
    "    note_array['id'] = np.array([f'n{i}' for i in range(len(generated_sequence))])\n",
    "\n",
    "    signal = synthesize(note_array, harmonic_dist='shepard')\n",
    "    ipd.display(\n",
    "            ipd.Audio(\n",
    "                data=signal, \n",
    "                rate=SAMPLE_RATE, \n",
    "                normalize=False\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    return note_array\n",
    "    \n",
    "n_notes = 100\n",
    "generated_sequence = generate(length=n_notes)\n",
    "print(generated_sequence)\n",
    "note_array = synthesize_generated_sequence(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6b324",
   "metadata": {},
   "source": [
    "## Tasks 5; n-gram Context Model:\n",
    "\n",
    "1. The Markov Chains used until now have only very limited memory. In fact, they only ever know the last played pitch or duration. Longer memory models can be created by using the conditional probability of any new symbol based on an n-gram context of the symbol (https://en.wikipedia.org/wiki/N-gram):\n",
    "$$ p(\\mathbf{x_i}\\mid \\mathbf{x_{i-1}}, \\dots, \\mathbf{x_{i-n}}) $$\n",
    "\n",
    "This probability will generally not look like a matrix anymore, but we can easily encode it as a dictionary. Write a function that creates a 3-gram context model from the data sequence **X**!\n",
    "\n",
    "2. The longer the context, the more data we need to get meaningful or even existing samples for all contexts (note that the number of different contexts grows exponentially with context length). What could we do to approximate the distribution for unseen contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "def create_context_model(sequences, n, n_states=alphabet_size):\n",
    "    a_priori_probability = np.ones(n_states)/n_states\n",
    "    context_model = defaultdict(lambda: copy.copy(a_priori_probability))\n",
    "    for sequence in sequences:\n",
    "        for idx in range(len(sequence)-n):\n",
    "            local_string = \"\"\n",
    "            for p in sequence[idx:idx+n]:\n",
    "                local_string += str(p)\n",
    "            context_model[local_string][sequence[idx+n]] += 1\n",
    "    \n",
    "    for key in context_model.keys():\n",
    "        prob_dist = context_model[key]\n",
    "        context_model[key] =  prob_dist/ prob_dist.sum()     \n",
    "    \n",
    "    return context_model\n",
    "            \n",
    "cm = create_context_model(encoded_sequences, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_context_model(start = [0,0,0], length = 100, context_model= cm):\n",
    "    melody = start\n",
    "    for k in range(length):\n",
    "        key = \"\"\n",
    "        for p in melody[-len(start):]:\n",
    "            key += str(p)\n",
    "        melody.append(sample(context_model[key]))\n",
    "    return np.array(melody)\n",
    "\n",
    "generated_sequence = generate_with_context_model()\n",
    "\n",
    "note_array = synthesize_generated_sequence(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92566c",
   "metadata": {},
   "source": [
    "## Tasks 6; multi-type Markov Chains and back to music:\n",
    "\n",
    "1. To generate a somewhat interesting melody, we want to get a sequence of both pitches and durations. If we encode rests too, we can generate any melody like this. So far our Markov Chains dealt with either pitch or duration/IOI. What could we do to combine them? Describe two approaches and why to choose which one.\n",
    "\n",
    "2. Implement a simple melody generator with pitch and IOI/duration (simplest; modify taska 4; 2 to a generator of the other type and use them to create independent seuqnces). Write some generated melodies to MIDI files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77888893",
   "metadata": {},
   "source": [
    "## (Tasks 7); more stuff for music:\n",
    "\n",
    "1. Keys are perceptual centers of gravity in the pitch space, so if we transpose all the input sequences to the same key we can compute empirical pitch distributions within a key!\n",
    "\n",
    "2. One solution to tasks 5, 2 is to use Prediction by Partial Matching. This is the basis of the most elaborate probabilitstic model ofsymbolic music the Information Dynamics of Music (IDyOM). See references here:\n",
    "https://researchcommons.waikato.ac.nz/bitstream/handle/10289/9913/uow-cs-wp-1993-12.pdf\n",
    "https://mtpearce.github.io/idyom/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
