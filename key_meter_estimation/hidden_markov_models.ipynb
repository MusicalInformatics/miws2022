{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "## Quick Recap of Probability\n",
    "\n",
    "* Probability: measure of the **likelihood** of an event\n",
    "\n",
    "* $0\\leq p(x) \\leq 1$, \n",
    "    * $p(x) = 0$ indicates that the event is very unlikely to occur \n",
    "    * $p(x) = 1$ indicates that the event will most likely occur\n",
    "    \n",
    "* \n",
    "    \n",
    "## Quick Recap: Graph Theory\n",
    "\n",
    "* Graphs: A tuple $\\mathcal{G}=(\\mathbf{X}, \\mathbf{E})$ consisting in a set of nodes $\\mathbf{X} = \\{X_1,\\dots,X_N\\}$ and a set of edges $\\mathbf{E}$ connecting the nodes.\n",
    "\n",
    "* Edges can be *directed* $(X_i \\rightarrow X_j)$ or *undirected* $(X_i - X_j)$. If the nodes of a graph are directed, we call it a *directed Graph*, otherwise is an *undirected graph*\n",
    "* Parent and Children nodes: In directed graphs, if $(X_i \\rightarrow X_j)\\in \\mathbf{E}$ then $X_i$ is a parent of $X_j$ and $X_j$ is a child of $X_i$. \n",
    "    * $\\mathbf{Pa}(X_i)$ is the set of parents of $X_i$\n",
    "    * $\\mathbf{Ch}(X_i)$ is the set of children of $X_i$\n",
    "    \n",
    "Let's consider this graph\n",
    "\n",
    "<div>\n",
    "<img src=\"img/graph_example.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "* the parents of $X_3$ are $\\mathbf{Pa}(X_3) = \\{X_1, X_2\\}$.\n",
    "* Which are the parents of $X_4$?\n",
    "* Which are the parents of $X_1$?\n",
    "\n",
    "\n",
    "## Quick Recap: Probabilistic Graphical Models\n",
    "\n",
    "* Probabilistic graphical models (PGMs) provide a way to visualize the structure of a probabilisitc model\n",
    "\n",
    "* Easy and elegant way to represent conditional independence properties\n",
    "\n",
    "* **Bayesian Networks**: Nodes in a graph represent *random variable* and edges specify conditional independence properties:\n",
    "\n",
    "$$p(X_1, \\dots, X_N) = \\prod_{i=1}^{N} p(X_i \\mid \\mathbf{Pa}(X_i))$$\n",
    "\n",
    "\n",
    "For the example above\n",
    "\n",
    "$$p(X_1, X_2, X_3, X_4, X_5, X_6) = p(X_1) p(X_2) p(X_3\\mid X_1, X_2) p(X_4 \\mid X_3) p(X_5 \\mid X_3) p(X_6 \\mid X_3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models\n",
    "\n",
    "* The simplest way of modeling a sequence of observations is to treat them as independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/independent_markov.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\mathbf{x}_1\\dots, \\mathbf{x}_N) = \\prod_{n=1}^{N} p(\\mathbf{x}_i)$$\n",
    "\n",
    "but this is a poor assumption for inherently sequential data (like music!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Easy way to model sequential data: the conditional distribution of each variable is independent of all previous observations except for the most recent: *first-order Markov chain*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/first_order_markov.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) = p(\\mathbf{x}_1)\\prod_{n=2}^{N}p(\\mathbf{x}_nÂ \\mid \\mathbf{x}_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "### Model Definition\n",
    "\n",
    "A hidden Markov Model (HMM) is a *state space model* that is not limited by the Markov assumption to any order. To do this, for each observation $\\mathbf{x}_n$, we have a corresponding *hidden* (latent) variable $\\mathbf{z}_n$ that satisfies the conditional independence property that $\\mathbf{z}_{n-1}$ and $\\mathbf{z}_{n+1}$ are independent give $\\mathbf{z}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/hmm_example.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since an HMM is a Bayesian network, its joint probability distribution is given by\n",
    "$$\n",
    "p(\\mathbf{x}_1, \\dots, \\mathbf{x}_{N-1}, \\mathbf{z}_{1},\\dots \\mathbf{z}_{N-1})=p(\\mathbf{z}_1) \\left[ \\prod_{n=2}^N p(\\mathbf{z}_n \\mid \\mathbf{z}_{n-1})\\right]\\prod_{n=1}^N p(\\mathbf{x}_n \\mid \\mathbf{z}_n)\n",
    "$$\n",
    "\n",
    "* In HMMs, the hidden variables $\\mathbf{z}_n$ are discrete and are typically represented by multinomial variables. Therefore, it is convenient to use a 1-of-$K$ coding scheme. \n",
    "\n",
    "* The conditional distribution $p(\\mathbf{z}_n \\mid \\mathbf{z}_{n -1})$ can be represented by a matrix $\\mathbf{A}$, commonly referred to as *transition probabilities*.\n",
    "\n",
    "$$A_{ij} = p(z_{ni} = 1 \\mid  z_{n-1,j} = 1)$$\n",
    "\n",
    "where $0\\leq A_ij \\leq 1$ and $\\sum_j A_{ij} = 1$.\n",
    "\n",
    "$$p(\\mathbf{z}_n \\mid \\mathbf{z}_{n-1}) = \\prod_{i=1}^K \\prod_{j=1}^K A_{ij}^{z_{n-1,j} \\cdot z_{n,i}}$$\n",
    "\n",
    "* Since $\\mathbf{z}_1$ has no parents, the marginal distribution $p(\\mathbf{z}_1)$ is represented by a vector of probabilities $\\mathbf{\\pi}$ with elements $\\pi_k = p(z_{1k} = 1)$\n",
    "\n",
    "$$p(\\mathbf{z}_1) = \\prod_{k=1}^K \\pi_k^{z_{1k}}$$\n",
    "\n",
    "* The distribution of the observed variables (*emission probabilities*) is modeled by $p(\\mathbf{x}_n \\mid \\mathbf{z}_n, \\mathbf{\\phi})$, where $\\mathbf{\\phi}$ is the set of parameters of this distribution.\n",
    "\n",
    "$$ p(\\mathbf{x}_n \\mid \\mathbf{z}_n) = \\prod_{k=1}^K p(\\mathbf{x}_n \\mid  \\mathbf{\\phi}_k)^{z_{nk}} $$\n",
    "\n",
    "* The set of parameters of the HMM is then $\\mathbf{\\theta} = \\{\\mathbf{\\pi}, \\mathbf{A}, \\mathbf{\\phi}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Weather forcasting\n",
    "\n",
    "Here is a classical exaple for HMMs!\n",
    "\n",
    "As mentioned above, we can fully specify the HMM by defining 3 element\n",
    "* an observation model that computes $p(\\mathbf{x}_n \\mid \\mathbf{z}_n)$\n",
    "* the transition probability matrix $\\mathbf{A}$\n",
    "* the initial probability vector $\\mathbf{\\pi}$\n",
    "\n",
    "We are going to use the [`hiddenmarkov` package](https://github.com/neosatrapahereje/hiddenmarkov), which we prepared to help you implement HMMs. (You might need to install it in your environment via `pip install python-hiddenmarkov`)\n",
    "\n",
    "Here is an example observation model, for the case that $\\mathbf{x}_i$ can take $M$ different states, and that the probability $p(\\mathbf{x}_n = m \\mid \\mathbf{z}_n = k) = c_{n,k}$ is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunny' 'Sunny' 'Raining']\n"
     ]
    }
   ],
   "source": [
    "obs = (\"normal\", \"jacket\", \"umbrella\")\n",
    "observations = (\"normal\", \"umbrella\", \"jacket\", \"umbrella\", \"normal\")\n",
    "states = (\"Sunny\", \"Raining\")\n",
    "observation_probabilities = np.array([[0.5, 0.1],\n",
    "                                      [0.4, 0.3],\n",
    "                                      [0.1, 0.6]])\n",
    "transition_probabilities = np.array([[0.7, 0.4],\n",
    "                                     [0.3, 0.6]])\n",
    "\n",
    "observation_model = CategoricalObservationModel(\n",
    "    observation_probabilities, obs\n",
    ")\n",
    "\n",
    "init_distribution = np.array([0.6, 0.4])\n",
    "\n",
    "best_sequence, log_likelihood = viterbi_algorithm(\n",
    "    obs, init_distribution, \n",
    "    observation_model, transition_probabilities,\n",
    "    state_space=np.array(states)\n",
    ")\n",
    "\n",
    "print(best_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(observation_model(\"normal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'b' 'a' 'b' 'a' 'b' 'a' 'b' 'a' 'b' 'a' 'b'] 3.600000000000001e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/miniconda3/envs/miws22/lib/python3.7/site-packages/hiddenmarkov/__init__.py:612: RuntimeWarning: divide by zero encountered in log\n",
      "  else:\n"
     ]
    }
   ],
   "source": [
    "from hiddenmarkov import ConstantTransitionModel, HMM\n",
    "\n",
    "class CategoricalStringObservationModel(ObservationModel):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_probabilities,\n",
    "            observations=None,\n",
    "            use_log_probabilities=True\n",
    "    ):\n",
    "        super().__init__(use_log_probabilities=use_log_probabilities)\n",
    "\n",
    "        self.observation_probabilities = observation_probabilities\n",
    "\n",
    "        if observations is not None:\n",
    "            self.observations = list(observations)\n",
    "        else:\n",
    "            self.observations = [\n",
    "                str(i) for i in range(len(observation_probabilities))\n",
    "            ]\n",
    "\n",
    "    @property\n",
    "    def observation_probabilities(self):\n",
    "        if self.use_log_probabilities:\n",
    "            return self._log_obs_prob\n",
    "        else:\n",
    "            return self._obs_prob\n",
    "\n",
    "    @observation_probabilities.setter\n",
    "    def observation_probabilities(self, observation_probabilities):\n",
    "        self._obs_prob = observation_probabilities\n",
    "        self._log_obs_prob = np.log(self._obs_prob)\n",
    "\n",
    "    def __call__(self, observation, *args, **kwargs):\n",
    "        idx = self.observations.index(observation)\n",
    "        return self.observation_probabilities[idx]\n",
    "\n",
    "    \n",
    "states = (\"a\",\"b\")\n",
    "observation_probabilities = np.array([[0.5, 0.1],\n",
    "                                          [0.4, 0.3],\n",
    "                                          [0.1, 0.6]])\n",
    "\n",
    "transition_probabilities = np.array([[0.0, 1.0],\n",
    "                                          [1.0, 0.0]])\n",
    "\n",
    "observation_model = CategoricalStringObservationModel(observation_probabilities )\n",
    "\n",
    "transition_model = ConstantTransitionModel(transition_probabilities)\n",
    "\n",
    "hmm = HMM(observation_model, transition_model, state_space=states)\n",
    "\n",
    "path, prob = hmm.find_best_sequence([str(k) for k in np.random.randint(3, size=(12))], log_probabilities=False)\n",
    "print(path, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Inference Problems with HMMs\n",
    "\n",
    "1. Given $\\mathbf{\\theta}$ and a sequence of observations, find the most likely sequence of hidden varibles\n",
    "    * Viterbi Algorithm\n",
    "    \n",
    "2. Given $\\mathbf{\\theta}$ and a sequence of observations, find the probability of the observed sequence\n",
    "    * Forward algorithm (not covered here)\n",
    "    \n",
    "3. Given sequences of observations, learn the model parameters $\\mathbf{\\theta}$\n",
    "    * Maximum likelihood Using Expectation-Maximization (also not covered here!)\n",
    "    \n",
    "For a more detailed (and formal) description, see the tutorial by [Pernkopf et al., 2013](https://www2.spsc.tugraz.at/www-archive/downloads/PGM.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "* In many applications, the hidden variables have some meaningful interpretation:\n",
    "    * In speech recognition: find the most probable sequence of phonemes\n",
    "    * In bioinformatics: Aligning DNA/RNA sequences\n",
    "    * In MIR: Music alignment, chord recognition, key identification...\n",
    "    \n",
    "* Formally, we would like to find\n",
    "\n",
    "$$\\hat{\\mathbf{Z}} = \\arg \\max_\\mathbf{Z} p(\\mathbf{X}, \\mathbf{Z})$$\n",
    "\n",
    "where $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ and $\\mathbf{Z} = \\{\\mathbf{z}_1,\\dots, \\mathbf{z}_N \\}$\n",
    "\n",
    "* Direct optimization of the joint distribution might not be feasible!\n",
    "\n",
    "* Viterbi Algorithm: Use dynamic programming (like in DTW!)\n",
    "\n",
    "* We define \n",
    "$$\\omega(\\mathbf{z}_n) = \\max_{\\mathbf{z}_1, \\dots, \\mathbf{z}_{n - 1}}\\log p(\\mathbf{x}_1,\\dots, \\mathbf{x}_{n-1}, \\mathbf{z}_1,\\dots, \\mathbf{z}_{n-1})$$\n",
    "\n",
    "(compare to the Dynamic Time Warping distance!)\n",
    "\n",
    "#### Algorithm \n",
    "\n",
    "**Inputs**  \n",
    "    \n",
    "* Sequence of observations $\\mathbf{X}=\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$\n",
    "* Input probabilities $\\mathbf{\\pi}$\n",
    "* Transition Matrix $\\mathbf{A}$\n",
    "* Observation model (to compute $p(\\mathbf{x}_{n} \\mid \\mathbf{z}_n)$)\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "We want to compute\n",
    "\n",
    "$$\\omega(\\mathbf{z}_1) =  \\log p(\\mathbf{x}_1 \\mid \\mathbf{z}_1) + \\log p(\\mathbf{z}_1)$$\n",
    "\n",
    "* For $k \\in [1, K]$\n",
    "    $$ \\omega_{1k} = p(\\mathbf{x}_n \\mid  \\mathbf{\\phi}_k) + \\log \\pi_k $$\n",
    "\n",
    "**Viterbi interation**\n",
    "\n",
    "We want to compute\n",
    "\n",
    "$$\\omega(\\mathbf{z}_n) = \\log p(\\mathbf{x}_{n} \\mid \\mathbf{z}_{n}) + \\max_{\\mathbf{z}_n} \\left\\{\\log p(\\mathbf{z}_{n} \\mid \\mathbf{z}_{n-1}) + \\omega(\\mathbf{z}_{n - 1})  \\right\\}$$\n",
    "\n",
    "* For $n\\in[2, N]$\n",
    "    * For $k \\in [1, K]$\n",
    "        $$\\omega_{nk} = p(\\mathbf{x}_n \\mid  \\mathbf{\\phi}_k) + \\max_i\\{ A_{ik} + \\omega_{n-1, i} \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(observations, init_distribution, \n",
    "                      observation_model, transition_probabilities,\n",
    "                      state_space=None):\n",
    "    \"\"\"\n",
    "    Find the most probable sequence of latent variables given\n",
    "    a sequence of observations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observations: iterable\n",
    "       An iterable containing observations. The type of each\n",
    "       element depends on input types accepted by the\n",
    "       `observation_model`\n",
    "    init_distribution: np.array\n",
    "        A 1D vector of length n_states defining the initial\n",
    "        probabilities of each state\n",
    "    observation_model : callable\n",
    "        A method for computiong the observation (emission) probabilities.\n",
    "    transition_probabilities: np.ndarray\n",
    "        A (n_states, n_states) matrix where component\n",
    "        [i, j] represents the probability of going to state j\n",
    "        from state i.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path: np.ndarray\n",
    "        The most probable sequence of latent variables\n",
    "    likelihood: float\n",
    "        The log-likelihood of the best sequence.\n",
    "    \"\"\"\n",
    "    # Initialize matrix for holding the best sub-sequence\n",
    "    # log-likelihood\n",
    "    n_states = len(transition_probabilities)\n",
    "    omega = np.zeros((len(observations), n_states))\n",
    "    # Initialize dictionary for tracking the best paths\n",
    "    path = defaultdict(lambda: list())\n",
    "    \n",
    "    # Initiate for i == 0\n",
    "    obs_prob = observation_model(observations[0])\n",
    "    omega[0, :] = obs_prob + init_distribution\n",
    "    # omega[0, :] = 0\n",
    "\n",
    "    # Initialize path\n",
    "    for i in range(n_states):\n",
    "        path[i].append(i)\n",
    "\n",
    "    # Viterbi recursion\n",
    "    for i, obs in enumerate(observations[1:], 1):\n",
    "        obs_prob = observation_model(obs)\n",
    "        for j in range(n_states):\n",
    "            # prob, state = 0, 0\n",
    "            prob, state = max(\n",
    "                [(omega[i - 1, k] + transition_probabilities[k, j], k)\n",
    "                 for k in range(n_states)], \n",
    "                key=lambda x: x[0]\n",
    "                )\n",
    "            omega[i, j] = obs_prob[j] + prob\n",
    "\n",
    "            # keep track of the best state\n",
    "            path[j].append(state)\n",
    "    \n",
    "    # Get index of the best state\n",
    "    best_sequence_idx = omega[-1, :].argmax()\n",
    "    # Get best path (backtracking!)\n",
    "    best_sequence = np.array(path[best_sequence_idx][::-1], dtype=int)\n",
    "    if state_space is not None:\n",
    "        best_sequence = state_space[best_sequence]\n",
    "    # likelihood of the path\n",
    "    path_likelihood = omega[-1, best_sequence_idx]\n",
    "\n",
    "    return best_sequence, path_likelihood"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
