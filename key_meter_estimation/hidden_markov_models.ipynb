{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "## Quick Recap of Probability\n",
    "\n",
    "* Probability: measure of the **likelihood** of an event\n",
    "\n",
    "* $0\\leq p(x) \\leq 1$, \n",
    "    * $p(x) = 0$ indicates that the event is very unlikely to occur \n",
    "    * $p(x) = 1$ indicates that the event will most likely occur\n",
    "    \n",
    "* Random variables: $X$ can take values $x$\n",
    "\n",
    "    \n",
    "## Quick Recap: Graph Theory\n",
    "\n",
    "* Graphs: A tuple $\\mathcal{G}=(\\mathbf{X}, \\mathbf{E})$ consisting in a set of nodes $\\mathbf{X} = \\{X_1,\\dots,X_N\\}$ and a set of edges $\\mathbf{E}$ connecting the nodes.\n",
    "\n",
    "* Edges can be *directed* $(X_i \\rightarrow X_j)$ or *undirected* $(X_i - X_j)$. If the nodes of a graph are directed, we call it a *directed Graph*, otherwise is an *undirected graph*\n",
    "* Parent and Children nodes: In directed graphs, if $(X_i \\rightarrow X_j)\\in \\mathbf{E}$ then $X_i$ is a parent of $X_j$ and $X_j$ is a child of $X_i$. \n",
    "    * $\\mathbf{Pa}(X_i)$ is the set of parents of $X_i$\n",
    "    * $\\mathbf{Ch}(X_i)$ is the set of children of $X_i$\n",
    "    \n",
    "Let's consider this graph\n",
    "\n",
    "<div>\n",
    "<img src=\"img/graph_example.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "* the parents of $X_3$ are $\\mathbf{Pa}(X_3) = \\{X_1, X_2\\}$.\n",
    "* Which are the parents of $X_4$?\n",
    "* Which are the parents of $X_1$?\n",
    "\n",
    "\n",
    "## Quick Recap: Probabilistic Graphical Models\n",
    "\n",
    "* Probabilistic graphical models (PGMs) provide a way to visualize the structure of a probabilisitc model\n",
    "\n",
    "* Easy and elegant way to represent conditional independence properties\n",
    "\n",
    "* **Bayesian Networks**: Nodes in a graph represent *random variable* and edges specify conditional independence properties:\n",
    "\n",
    "$$p(X_1, \\dots, X_N) = \\prod_{i=1}^{N} p(X_i \\mid \\mathbf{Pa}(X_i))$$\n",
    "\n",
    "\n",
    "For the example above\n",
    "\n",
    "$$p(X_1, X_2, X_3, X_4, X_5, X_6) = p(X_1) p(X_2) p(X_3\\mid X_1, X_2) p(X_4 \\mid X_3) p(X_5 \\mid X_3) p(X_6 \\mid X_3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models\n",
    "\n",
    "* The simplest way of modeling a sequence of observations is to treat them as independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/independent_markov.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\mathbf{x}_1\\dots, \\mathbf{x}_N) = \\prod_{n=1}^{N} p(\\mathbf{x}_i)$$\n",
    "\n",
    "but this is a poor assumption for inherently sequential data (like music!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Easy way to model sequential data: the conditional distribution of each variable is independent of all previous observations except for the most recent: *first-order Markov chain*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/first_order_markov.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\mathbf{x}_1, \\dots, \\mathbf{x}_N) = p(\\mathbf{x}_1)\\prod_{n=2}^{N}p(\\mathbf{x}_nÂ \\mid \\mathbf{x}_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "### Model Definition\n",
    "\n",
    "A hidden Markov Model (HMM) is a *state space model* that is not limited by the Markov assumption to any order. To do this, for each observation $\\mathbf{x}_n$, we have a corresponding *hidden* (latent) variable $\\mathbf{z}_n$ that satisfies the conditional independence property that $\\mathbf{z}_{n-1}$ and $\\mathbf{z}_{n+1}$ are independent give $\\mathbf{z}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/hmm_example.png\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since an HMM is a Bayesian network, its joint probability distribution is given by\n",
    "$$\n",
    "p(\\mathbf{x}_1, \\dots, \\mathbf{x}_{N-1}, \\mathbf{z}_{1},\\dots \\mathbf{z}_{N-1})=p(\\mathbf{z}_1) \\left[ \\prod_{n=2}^N p(\\mathbf{z}_n \\mid \\mathbf{z}_{n-1})\\right]\\prod_{n=1}^N p(\\mathbf{x}_n \\mid \\mathbf{z}_n)\n",
    "$$\n",
    "\n",
    "* In HMMs, the hidden variables $\\mathbf{z}_n$ are discrete and are typically represented by multinomial variables. Therefore, it is convenient to use a 1-of-$K$ coding scheme. \n",
    "\n",
    "* The conditional distribution $p(\\mathbf{z}_n \\mid \\mathbf{z}_{n -1})$ can be represented by a matrix $\\mathbf{A}$, commonly referred to as *transition probabilities*.\n",
    "\n",
    "$$A_{ij} = p(z_{ni} = 1 \\mid  z_{n-1,j} = 1)$$\n",
    "\n",
    "where $0\\leq A_ij \\leq 1$ and $\\sum_j A_{ij} = 1$.\n",
    "\n",
    "$$p(\\mathbf{z}_n \\mid \\mathbf{z}_{n-1}) = \\prod_{i=1}^K \\prod_{j=1}^K A_{ij}^{z_{n-1,j} \\cdot z_{n,i}}$$\n",
    "\n",
    "* Since $\\mathbf{z}_1$ has no parents, the marginal distribution $p(\\mathbf{z}_1)$ is represented by a vector of probabilities $\\mathbf{\\pi}$ with elements $\\pi_k = p(z_{1k} = 1)$\n",
    "\n",
    "$$p(\\mathbf{z}_1) = \\prod_{k=1}^K \\pi_k^{z_{1k}}$$\n",
    "\n",
    "* The distribution of the observed variables (*emission probabilities*) is modeled by $p(\\mathbf{x}_n \\mid \\mathbf{z}_n, \\mathbf{\\phi})$, where $\\mathbf{\\phi}$ is the set of parameters of this distribution.\n",
    "\n",
    "$$ p(\\mathbf{x}_n \\mid \\mathbf{z}_n) = \\prod_{k=1}^K p(\\mathbf{x}_n \\mid  \\mathbf{\\phi}_k)^{z_{nk}} $$\n",
    "\n",
    "* The set of parameters of the HMM is then $\\mathbf{\\theta} = \\{\\mathbf{\\pi}, \\mathbf{A}, \\mathbf{\\phi}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Weather forcasting\n",
    "\n",
    "As mentioned above, we can fully specify the HMM by defining 3 element\n",
    "* an observation model that computes $p(\\mathbf{x}_n \\mid \\mathbf{z}_n)$\n",
    "* the transition probability matrix $\\mathbf{A}$\n",
    "* the initial probability vector $\\mathbf{\\pi}$\n",
    "\n",
    "Let's start with a classical exaple for HMMs!\n",
    "\n",
    "Imagine that you are working in a recording studio, and you spend most of the time in a **room without windows**. For the next few months you are going to be working with the same musicians, and you notice that some days they are wearing **jackets** and some other days they are bringing **umbrellas**! You start wondering if it is possible to know whether it is raning based on your observations of what the musicians are wearing.\n",
    "\n",
    "Let's start by codifying our observed states and hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Iterable, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the possible observed states\n",
    "observed_states = (\n",
    "    \"normal\", # No jacket, no umbrella\n",
    "    \"jacket\",\n",
    "    \"umbrella\",\n",
    ")\n",
    "\n",
    "# Hidden (latent) states\n",
    "hidden_states = (\n",
    "    \"Sunny\", # today is sunny\n",
    "    \"Raining\", # today is raining\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After keeping track of the weather for a few weeks, we come to the following conditional probabilities:\n",
    "\n",
    "(formally, in this case $\\mathbf{x}_i$ can take $M=3$ different states and the probability $p(\\mathbf{x}_n = m \\mid \\mathbf{z}_n = k) = c_{n,k}$ is constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_probabilities = np.array(\n",
    "    [\n",
    "        [0.5, 0.1], # Probability of observing \"normal\" if \"Sunny\"/\"Raining\"\n",
    "        [0.4, 0.3], # Probability of observing \"jacket\" if \"Sunny\"/\"Raining\"\n",
    "        [0.1, 0.6], # Probability of observing \"umbrella\" if \"Sunny\"/\"Raining\"\n",
    "    ]\n",
    ")\n",
    "transition_probabilities = np.array(\n",
    "    [\n",
    "        [0.7, 0.4], # P(today is \"Sunny\" | yesterday was \"Sunny\"), P(today is \"Sunny\" | yesterday was \"Raining\")\n",
    "        [0.3, 0.6], # P(today is \"Raining\" | yesterday was \"Sunny\"), P(today is \"Raining\" | yesterday was \"Raining\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "init_distribution = np.array(\n",
    "    [\n",
    "        0.6, # Probability that any given day is \"Sunny\" \n",
    "        0.4, # Probability that any given day is \"Raining\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [`hiddenmarkov` package](https://github.com/neosatrapahereje/hiddenmarkov), which we prepared to help you implement HMMs. (You might need to install it in your environment via `pip install python-hiddenmarkov`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiddenmarkov import CategoricalObservationModel, ConstantTransitionModel, HMM\n",
    "\n",
    "observation_model = CategoricalObservationModel(\n",
    "    observation_probabilities, \n",
    "    observed_states,\n",
    "    use_log_probabilities=False,\n",
    ")\n",
    "\n",
    "transition_model = ConstantTransitionModel(transition_probabilities, \n",
    "                                           init_probabilities=init_distribution, \n",
    "                                           use_log_probabilities=False)\n",
    "\n",
    "# With the observation and transition models we can specify an HMM!\n",
    "hmm = HMM(observation_model, transition_model, state_space=np.array(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = \"normal\"\n",
    "# The observation model gives us the probability of an observation given the hidden states\n",
    "print(observation_model(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transition model tells us the proability of the current state given the previous state\n",
    "transition_model(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Inference Problems with HMMs\n",
    "\n",
    "1. Given $\\mathbf{\\theta}$ and a sequence of observations, find the most likely sequence of hidden varibles\n",
    "    * Viterbi Algorithm\n",
    "    \n",
    "2. Given $\\mathbf{\\theta}$ and a sequence of observations, find the probability of the observed sequence\n",
    "    * Forward algorithm (not covered here)\n",
    "    \n",
    "3. Given sequences of observations, learn the model parameters $\\mathbf{\\theta}$\n",
    "    * Maximum likelihood Using Expectation-Maximization (also not covered here!)\n",
    "    \n",
    "For a more detailed (and formal) description, see the tutorial by [Pernkopf et al., 2013](https://www2.spsc.tugraz.at/www-archive/downloads/PGM.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "* In many applications, the hidden variables have some meaningful interpretation:\n",
    "    * In speech recognition: find the most probable sequence of phonemes\n",
    "    * In bioinformatics: Aligning DNA/RNA sequences\n",
    "    * In MIR: Music alignment, chord recognition, key identification...\n",
    "    \n",
    "* Formally, we would like to find\n",
    "\n",
    "$$\\hat{\\mathbf{Z}} = \\arg \\max_\\mathbf{Z} p(\\mathbf{X}, \\mathbf{Z})$$\n",
    "\n",
    "where $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ and $\\mathbf{Z} = \\{\\mathbf{z}_1,\\dots, \\mathbf{z}_N \\}$\n",
    "\n",
    "* Direct optimization of the joint distribution might not be feasible!\n",
    "\n",
    "* Viterbi Algorithm: Use dynamic programming (like in DTW!)\n",
    "\n",
    "* We define\n",
    "$$\\omega(\\mathbf{z}_n) = \\max_{\\mathbf{z}_1, \\dots, \\mathbf{z}_{n - 1}}\\log p(\\mathbf{x}_1,\\dots, \\mathbf{x}_{n-1}, \\mathbf{z}_1,\\dots, \\mathbf{z}_{n-1})$$\n",
    "\n",
    "(compare to the Dynamic Time Warping distance!)\n",
    "\n",
    "#### Algorithm \n",
    "\n",
    "**Inputs**  \n",
    "    \n",
    "* Sequence of observations $\\mathbf{X}=\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$\n",
    "* Input probabilities $\\mathbf{\\pi}$\n",
    "* Transition Matrix $\\mathbf{A}$\n",
    "* Observation model (to compute $p(\\mathbf{x}_{n} \\mid \\mathbf{z}_n)$)\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "We want to compute\n",
    "\n",
    "$$\\omega(\\mathbf{z}_1) =  \\log p(\\mathbf{x}_1 \\mid \\mathbf{z}_1) + \\log p(\\mathbf{z}_1)$$\n",
    "\n",
    "* For $k \\in [1, K]$\n",
    "    $$ \\omega_{1k} = p(\\mathbf{x}_1 \\mid  \\mathbf{\\phi}_k) + \\log \\pi_k $$\n",
    "\n",
    "**Viterbi interation**\n",
    "\n",
    "We want to compute\n",
    "\n",
    "$$\\omega(\\mathbf{z}_n) = \\log p(\\mathbf{x}_{n} \\mid \\mathbf{z}_{n}) + \\max_{\\mathbf{z}_n} \\left\\{\\log p(\\mathbf{z}_{n} \\mid \\mathbf{z}_{n-1}) + \\omega(\\mathbf{z}_{n - 1})  \\right\\}$$\n",
    "\n",
    "* For $n\\in[2, N]$\n",
    "    * For $k \\in [1, K]$\n",
    "        $$\\omega_{nk} = p(\\mathbf{x}_n \\mid  \\mathbf{\\phi}_k) + \\max_i\\{ A_{ik} + \\omega_{n-1, i} \\}$$\n",
    "        \n",
    "        \n",
    "Let's put this together into a single function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(\n",
    "    hmm: HMM,\n",
    "    observations: Iterable,\n",
    "    log_probabilities: bool = True,\n",
    "    return_omega: bool = False,\n",
    ") -> Union[Tuple[np.ndarray, float], Tuple[np.ndarray, float, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Find the most probable sequence of latent variables given\n",
    "    a sequence of observations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observations: iterable\n",
    "       An iterable containing observations. The type of each\n",
    "       element depends on input types accepted by the\n",
    "       `hmm.observation_model`\n",
    "    log_probabilities: Bool (optional)\n",
    "       If True, uses log probabilities to compute the Viterbi\n",
    "       recursion (better for numerical stability). Default is True.\n",
    "    return_omega: bool \n",
    "       If True, return omega\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path: np.ndarray\n",
    "        The most probable sequence of latent variables\n",
    "    likelihood: float\n",
    "        The likelihood (either the probability or the\n",
    "        log proability if `log_probabilities` is True)\n",
    "        of the best sequence.\n",
    "    omega : np.ndarray\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    This is a naÃ¯ve implementation, mostly for educational purposes!\n",
    "    \"\"\"\n",
    "    # Set whether to use log probabilities in transition and\n",
    "    # observation models\n",
    "    hmm.transition_model.use_log_probabilities = log_probabilities\n",
    "    hmm.observation_model.use_log_probabilities = log_probabilities\n",
    "    # Initialize matrix for holding the best sub-sequence\n",
    "    # (log-)likelihood\n",
    "    omega = np.zeros((len(observations), hmm.n_states))\n",
    "    # Initialize dictionary for tracking the best paths\n",
    "    path = defaultdict(lambda: list())\n",
    "\n",
    "    # Initiate for i == 0\n",
    "    obs_prob = hmm.observation_model(observations[0])\n",
    "\n",
    "    if log_probabilities:\n",
    "        omega[0, :] = obs_prob + hmm.transition_model.init_probabilities\n",
    "    else:\n",
    "        omega[0, :] = obs_prob * hmm.transition_model.init_probabilities\n",
    "\n",
    "    # Viterbi recursion\n",
    "    for i, obs in enumerate(observations[1:], 1):\n",
    "        obs_prob = hmm.observation_model(obs)\n",
    "        for j in range(hmm.n_states):\n",
    "            if log_probabilities:\n",
    "                prob, state = max(\n",
    "                    [\n",
    "                        (omega[i - 1, k] + hmm.transition_model(k, j), k)\n",
    "                        for k in range(hmm.n_states)\n",
    "                    ],\n",
    "                    key=lambda x: x[0],\n",
    "                )\n",
    "                omega[i, j] = obs_prob[j] + prob\n",
    "\n",
    "            else:\n",
    "                prob, state = max(\n",
    "                    [\n",
    "                        (omega[i - 1, k] * hmm.transition_model(k, j), k)\n",
    "                        for k in range(hmm.n_states)\n",
    "                    ],\n",
    "                    key=lambda x: x[0],\n",
    "                )\n",
    "                omega[i, j] = obs_prob[j] * prob\n",
    "            # keep track of the best state\n",
    "            path[j].append(state)\n",
    "\n",
    "    # Get best path (backtracking!)\n",
    "    # Get index of the best state\n",
    "    best_sequence_idx = omega[-1, :].argmax()\n",
    "    # likelihood of the path\n",
    "    path_likelihood = omega[-1, best_sequence_idx]\n",
    "    # follow the best path backwards\n",
    "    seq = [best_sequence_idx]\n",
    "    for s in range(len(path[best_sequence_idx])):\n",
    "        best_sequence_idx = path[best_sequence_idx][-(s + 1)]\n",
    "        seq.append(best_sequence_idx)\n",
    "    # invert the path\n",
    "    best_sequence = np.array(seq[::-1], dtype=int)\n",
    "\n",
    "    if hmm.state_space is not None:\n",
    "        best_sequence = hmm.state_space[best_sequence]\n",
    "        \n",
    "    if return_omega:\n",
    "        return best_sequence, path_likelihood, omega\n",
    "\n",
    "    return best_sequence, path_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Report!\n",
    "\n",
    "Let's go back to our example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [\"umbrella\", \"umbrella\", \"umbrella\", \"jacket\", \"normal\", \"normal\", \"jacket\"]\n",
    "\n",
    "best_sequence, likelihood, omega = viterbi_algorithm(hmm, observations, \n",
    "                                                     log_probabilities=True, \n",
    "                                                     return_omega=True)\n",
    "\n",
    "best_seq_indices = np.array([hidden_states.index(bs) for bs in best_sequence])\n",
    "\n",
    "observation_probs = np.array([hmm.observation_model(obs) for obs in observations])\n",
    "\n",
    "print(\"Best sequence:\")\n",
    "print(best_sequence)\n",
    "\n",
    "fig, axes = plt.subplots(2, sharex=True)\n",
    "axes[0].plot(np.arange(len(observations)), best_seq_indices, \n",
    "             marker='d', color='red', linewidth=3)\n",
    "axes[1].plot(np.arange(len(observations)), best_seq_indices, \n",
    "             marker='d', color='red', linewidth=3)\n",
    "axes[0].imshow(observation_probs.T, \n",
    "               aspect=\"equal\", origin=\"lower\", cmap=\"gray\")\n",
    "axes[1].set_yticks(range(2))\n",
    "axes[1].set_yticklabels([\"Sunny\", \"Raining\"])\n",
    "axes[1].imshow(omega.T, aspect='equal', origin=\"lower\", cmap=\"gray\")\n",
    "axes[1].set_xticks(range(len(observations)))\n",
    "axes[1].set_xticklabels([f\"day {i + 1}\" for i in range(len(observations))])\n",
    "axes[1].set_yticks(range(2))\n",
    "axes[1].set_yticklabels([\"Sunny\", \"Raining\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
